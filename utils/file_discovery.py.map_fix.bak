"""
# module: file_discovery
File Discovery System

This module provides reliable methods for discovering CSV files on game servers
via SFTP. It ensures consistent behavior and proper tracking of discovered files.
"""
import logging
import os
import re
from datetime import datetime, timedelta
from typing import List, Dict, Set, Any, Optional, Tuple

# Import SFTP manager but avoid circular imports
from utils.sftp import SFTPManager

# Setup logging
logger = logging.getLogger(__name__)

class FileDiscovery:
    """
    Centralized, reliable file discovery for CSV files via SFTP.
    
    This class handles discovering CSV files on game servers through SFTP,
    properly tracks discovered files, and maintains state about what's been
    found and where.
    """
    
    def __init__(self):
        """Initialize the file discovery system."""
        # Maps server IDs to discovered files
        self.discovered_files: Dict[str, Set[str]] = {}
        # Maps server IDs to map directory files
        self.map_directory_files: Dict[str, Set[str]] = {}
        # Records of all possible directories
        self.known_directories: Dict[str, List[str]] = {}
        # Indicator for if we've run a check on a server
        self.checked_servers: Set[str] = set()
        
    async def discover_csv_files(self, 
                               sftp: SFTPManager, 
                               server_id: str,
                               start_date: Optional[datetime] = None,
                               days_back: int = 1,
                               historical_mode: bool = False) -> Tuple[List[str], Dict[str, Any]]:
        """
        Discover CSV files for a specific server.
        
        Args:
            sftp: The SFTP manager to use for file discovery
            server_id: The ID of the server to discover files for
            start_date: The date to start searching from (default: 24 hours ago)
            days_back: Number of days to look back (default: 1)
            historical_mode: Whether to run in historical mode (more thorough search)
            
        Returns:
            Tuple containing (list of discovered files, metadata about the discovery)
        """
        if not sftp or not server_id:
            logger.error("Invalid SFTP manager or server ID provided")
            return [], {"error": "Invalid SFTP manager or server ID"}
            
        # Initialize the server's tracking if not already done
        if server_id not in self.discovered_files:
            self.discovered_files[server_id] = set()
            
        if server_id not in self.map_directory_files:
            self.map_directory_files[server_id] = set()
            
        # Set the start date to search from
        if not start_date:
            start_date = datetime.now() - timedelta(days=days_back)
            
        logger.info(f"Discovering CSV files for server {server_id} from {start_date}")
        
        # Find all possible base paths
        base_paths = await self._find_base_paths(sftp, server_id, historical_mode)
        
        # Find map directories
        map_directories = await self._find_map_directories(sftp, base_paths)
        
        # Build the pattern to match CSV files
        csv_pattern = r'.*\.csv$'
        
        # Always properly initialize these variables to prevent LSP errors
        all_discovered_files = []
        map_files = []
        regular_files = []
        
        # Search in base paths first
        for base_path in base_paths:
            path_files = await self._list_csv_files(sftp, base_path, csv_pattern)
            if path_files:
                logger.info(f"Found {len(path_files)} CSV files in base path {base_path}")
                all_discovered_files.extend(path_files)
                regular_files.extend(path_files)
                
        # Then search in map directories
        for map_dir in map_directories:
            map_path_files = await self._list_csv_files(sftp, map_dir, csv_pattern)
            if map_path_files:
                logger.info(f"Found {len(map_path_files)} CSV files in map directory {map_dir}")
                all_discovered_files.extend(map_path_files)
                map_files.extend(map_path_files)
                # Add to the map directory tracking
                self.map_directory_files[server_id].update(map_path_files)
                
        # Update the discovered files for this server
        all_file_paths = set(all_discovered_files)
        self.discovered_files[server_id].update(all_file_paths)
        
        # Filter files by date if needed
        if not historical_mode:
            filtered_files = self._filter_files_by_date(all_discovered_files, start_date)
        else:
            filtered_files = all_discovered_files
            
        # Mark this server as checked
        self.checked_servers.add(server_id)
        
        # Return relevant statistics
        discovery_stats = {
            "total_files": len(all_discovered_files),
            "map_files": len(map_files),  # Files found in map directories
            "map_directories": len(map_directories),  # Count of map directories found
            "regular_files": len(regular_files),
            "filtered_files": len(filtered_files),
            "start_date": start_date,
            "historical_mode": historical_mode
        }
        
        logger.info(f"Total files discovered for server {server_id}: {len(all_discovered_files)}")
        logger.info(f"Files matching date filter: {len(filtered_files)}")
        
        return filtered_files, discovery_stats
    
    async def _find_base_paths(self, sftp: SFTPManager, server_id: str, historical_mode: bool) -> List[str]:
        """
        Find all possible base paths to search for CSV files.
        
        Args:
            sftp: The SFTP manager to use
            server_id: The server ID
            historical_mode: Whether to use more extensive search paths
            
        Returns:
            List of base paths to search
        """
        # Start with standard paths for death logs
        base_paths = [
            "/deathlogs",
            "/logs/deathlogs",
            "/game/deathlogs",
            "/data/deathlogs",
            "/killfeed",
            "/logs/killfeed",
            "/game/logs",
            "/logs"
        ]
        
        # Add server-specific paths if we have server ID
        if server_id:
            server_paths = [
                f"/logs/{server_id}",
                f"/deathlogs/{server_id}",
                f"/killfeed/{server_id}",
                f"/game/logs/{server_id}",
                f"/data/logs/{server_id}"
            ]
            base_paths.extend(server_paths)
            
        # If in historical mode, add even more possibilities
        if historical_mode:
            extra_paths = [
                "/",
                "/game",
                "/data",
                "/var/log",
                "/var/log/game",
                "/home/logs"
            ]
            base_paths.extend(extra_paths)
            
        # Cache these paths for future use
        self.known_directories[server_id] = base_paths
        
        return base_paths
    
    async def _find_map_directories(self, sftp: SFTPManager, base_paths: List[str]) -> List[str]:
        """
        Find map directories that might contain CSV files.
        
        Args:
            sftp: The SFTP manager to use
            base_paths: List of base paths to search within
            
        Returns:
            List of map directories found
        """
        # Known map directory names, expanded for better coverage
        map_subdirs = [
            "world_0", "world0", "world_1", "world1", "world", "world2", "world_2",
            "map_0", "map0", "map_1", "map1", "map", "map2", "map_2",
            "main", "default", "maps", "custom_maps", "gamedata", "levels"
        ]
        
        logger.info(f"Searching for map directories in {len(base_paths)} base paths")
        
        map_dirs = []
        
        # Check each base path for map directories
        for base_path in base_paths:
            # First, directly check for known map directories
            for map_subdir in map_subdirs:
                map_path = os.path.join(base_path, map_subdir)
                try:
                    # Check if this is a valid directory
                    if await sftp.directory_exists(map_path):
                        logger.info(f"Found map directory: {map_path}")
                        map_dirs.append(map_path)
                        # Log this important finding at info level, not just debug
                except Exception as e:
                    logger.debug(f"Error checking map directory {map_path}: {e}")
                    continue
                    
            # Then try to list directories and look for patterns
            try:
                entries = await sftp.listdir(base_path)
                for entry in entries:
                    # Skip obvious non-map entries
                    if entry.startswith('.') or entry in ['logs', 'config', 'backups']:
                        continue
                        
                    # Check if it matches known map naming patterns
                    if re.search(r'(world|map|level|zone|region)[-_]?\d*', entry, re.IGNORECASE):
                        entry_path = os.path.join(base_path, entry)
                        try:
                            if await sftp.directory_exists(entry_path):
                                logger.debug(f"Found map directory by pattern: {entry_path}")
                                map_dirs.append(entry_path)
                        except Exception:
                            continue
            except Exception as e:
                logger.debug(f"Error listing directory {base_path}: {e}")
                continue
                
        return map_dirs
    
    async def _list_csv_files(self, sftp: SFTPManager, directory: str, pattern: str) -> List[str]:
        """
        List CSV files in a directory matching a pattern.
        
        Args:
            sftp: The SFTP manager to use
            directory: The directory to search in
            pattern: Regex pattern to match files against
            
        Returns:
            List of full paths to matching files
        """
        try:
            # Check if the directory exists first
            if not await sftp.directory_exists(directory):
                return []
                
            # List files matching the pattern
            files = await sftp.list_files(directory, pattern)
            
            # Convert to full paths if not already
            full_paths = []
            for file in files:
                if directory.endswith('/'):
                    path = f"{directory}{file}"
                else:
                    path = f"{directory}/{file}"
                full_paths.append(path)
                
            return full_paths
        except Exception as e:
            logger.error(f"Error listing CSV files in {directory}: {e}")
            return []
    
    def _filter_files_by_date(self, files: List[str], start_date: datetime) -> List[str]:
        """
        Filter files by date based on filename.
        
        Args:
            files: List of file paths to filter
            start_date: Minimum date to include
            
        Returns:
            Filtered list of file paths
        """
        filtered_files = []
        date_patterns = [
            # YYYY.MM.DD-HH.MM.SS
            r'(\d{4})\.(\d{2})\.(\d{2})-(\d{2})\.(\d{2})\.(\d{2})',
            # YYYY-MM-DD-HH-MM-SS
            r'(\d{4})-(\d{2})-(\d{2})-(\d{2})-(\d{2})-(\d{2})',
            # YYYY-MM-DD HH:MM:SS
            r'(\d{4})-(\d{2})-(\d{2}) (\d{2}):(\d{2}):(\d{2})'
        ]
        
        for file_path in files:
            file_name = os.path.basename(file_path)
            
            # Try to extract date from filename
            file_date = None
            for pattern in date_patterns:
                match = re.search(pattern, file_name)
                if match:
                    try:
                        year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
                        hour, minute, second = int(match.group(4)), int(match.group(5)), int(match.group(6))
                        file_date = datetime(year, month, day, hour, minute, second)
                        break
                    except (ValueError, IndexError):
                        continue
            
            # If we couldn't extract date, include the file conservatively
            if not file_date:
                filtered_files.append(file_path)
                continue
                
            # Check if file date is after start date
            if file_date >= start_date:
                filtered_files.append(file_path)
                
        return filtered_files
    
    def get_discovered_files(self, server_id: str) -> List[str]:
        """
        Get all discovered files for a server.
        
        Args:
            server_id: The server ID
            
        Returns:
            List of all discovered files for this server
        """
        return list(self.discovered_files.get(server_id, set()))
    
    def get_map_directory_files(self, server_id: str) -> List[str]:
        """
        Get all files found in map directories for a server.
        
        Args:
            server_id: The server ID
            
        Returns:
            List of all files found in map directories
        """
        return list(self.map_directory_files.get(server_id, set()))
    
    def reset_discovery_for_server(self, server_id: str):
        """
        Reset discovery tracking for a specific server.
        
        Args:
            server_id: The server ID to reset
        """
        if server_id in self.discovered_files:
            self.discovered_files[server_id] = set()
            
        if server_id in self.map_directory_files:
            self.map_directory_files[server_id] = set()
            
        if server_id in self.checked_servers:
            self.checked_servers.remove(server_id)