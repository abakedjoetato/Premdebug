"""
# module: csv_processor_coordinator
CSV Processor Coordinator

This module coordinates between the historical and killfeed parsers,
ensuring they work together correctly and don't interfere with each other.
"""
import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set

# Import our stable utilities
from utils.stable_csv_parser import StableCSVParser
from utils.file_discovery import FileDiscovery
from utils.sftp import SFTPManager

# Setup logging
logger = logging.getLogger(__name__)

class CSVProcessorCoordinator:
    """
    Coordinates CSV processing between historical and killfeed parsers.
    
    This class ensures that the two parsers work together correctly,
    prevents race conditions, and maintains shared state.
    """
    
    def __init__(self):
        """Initialize the coordinator."""
        # Create shared components
        self.parser = StableCSVParser()
        self.file_discovery = FileDiscovery()
        
        # State tracking
        self.processing_lock: Dict[str, asyncio.Lock] = {}
        self.last_processed: Dict[str, datetime] = {}
        self.historical_running: Set[str] = set()
        self.killfeed_pause: Dict[str, bool] = {}
        
        # Statistics tracking
        self.processing_stats: Dict[str, Dict[str, Any]] = {}
        
    async def get_lock_for_server(self, server_id: str) -> asyncio.Lock:
        """
        Get a dedicated lock for a server.
        
        Args:
            server_id: The server ID
            
        Returns:
            Lock for this server
        """
        if server_id not in self.processing_lock:
            self.processing_lock[server_id] = asyncio.Lock()
        return self.processing_lock[server_id]
    
    async def process_historical(self, 
                               sftp: SFTPManager, 
                               server_id: str,
                               days: int = 30) -> Tuple[int, int]:
        """
        Run historical processing for a server.
        
        Args:
            sftp: SFTP manager to use
            server_id: Server ID to process
            days: Number of days to look back
            
        Returns:
            Tuple of (files processed, events processed)
        """
        # Get the lock for this server
        lock = await self.get_lock_for_server(server_id)
        
        # Track statistics
        files_processed = 0
        events_processed = 0
        
        try:
            # Acquire the lock to prevent killfeed parser from running
            async with lock:
                # Signal historical parser is running
                self.historical_running.add(server_id)
                self.killfeed_pause[server_id] = True
                
                logger.info(f"Starting historical processing for server {server_id}, looking back {days} days")
                
                # Calculate the start date
                start_date = datetime.now() - timedelta(days=days)
                
                # Discover files using historical mode
                csv_files, discovery_stats = await self.file_discovery.discover_csv_files(
                    sftp=sftp,
                    server_id=server_id,
                    start_date=start_date,
                    days_back=days,
                    historical_mode=True
                )
                
                # Log discovery results
                total_files = discovery_stats["total_files"]
                map_files = discovery_stats["map_files"]
                regular_files = discovery_stats["regular_files"]
                
                logger.info(f"MAP STATS: Historical discovery found {total_files} total files " 
                          f"({map_files} in map dirs, {regular_files} in regular dirs)")
                
                # Process each file from the beginning
                for file_path in csv_files:
                    try:
                        content = await sftp.read_file(file_path)
                        if not content:
                            logger.warning(f"Empty content for file {file_path}")
                            continue
                            
                        # Process from the start (line 0)
                        events, total_lines = self.parser.parse_file_content(
                            content=content,
                            file_path=file_path,
                            server_id=server_id,
                            start_line=0
                        )
                        
                        # If we have events, process them
                        if events:
                            files_processed += 1
                            events_processed += len(events)
                            logger.info(f"Processed {len(events)} events from {file_path}")
                            
                            # Process events (to be implemented by the cog)
                            await self._process_events(events, server_id)
                            
                        # Mark this file as fully processed
                        self.parser.mark_file_as_processed(file_path)
                    except Exception as e:
                        logger.error(f"Error processing file {file_path}: {e}")
                        continue
                
                # Set the last processed time to now
                self.last_processed[server_id] = datetime.now()
                
                # Update processing statistics
                self.processing_stats[server_id] = {
                    "historical_last_run": datetime.now(),
                    "historical_files_processed": files_processed,
                    "historical_events_processed": events_processed,
                    "historical_days": days,
                    "files_found": total_files,
                    "map_files_found": len(map_files),
                    "regular_files_found": regular_files,
                    "map_directories_found": discovery_stats.get("map_directories", 0)
                }
                
                logger.info(f"Historical processing completed for server {server_id}. "
                          f"Processed {files_processed} files with {events_processed} events.")
                
                return files_processed, events_processed
        finally:
            # Ensure we always clear the historical running flag
            if server_id in self.historical_running:
                self.historical_running.remove(server_id)
                
            # Resume killfeed processing
            self.killfeed_pause[server_id] = False
    
    async def process_killfeed(self, 
                             sftp: SFTPManager, 
                             server_id: str) -> Tuple[int, int]:
        """
        Run killfeed processing for a server.
        
        Args:
            sftp: SFTP manager to use
            server_id: Server ID to process
            
        Returns:
            Tuple of (files processed, events processed)
        """
        # Check if we should pause for historical processing
        if server_id in self.historical_running or self.killfeed_pause.get(server_id, False):
            logger.info(f"Skipping killfeed processing for server {server_id} - historical processing is running")
            return 0, 0
            
        # Get the lock for this server
        lock = await self.get_lock_for_server(server_id)
        
        # Track statistics
        files_processed = 0
        events_processed = 0
        
        # Get a non-blocking lock if possible
        if lock.locked():
            logger.info(f"Skipping killfeed processing for server {server_id} - lock is held")
            return 0, 0
            
        try:
            # Try to acquire the lock without blocking
            if await asyncio.wait_for(lock.acquire(), timeout=0.1):
                try:
                    logger.info(f"Starting killfeed processing for server {server_id}")
                    
                    # Calculate the start date (last processed or 24 hours ago)
                    start_date = self.last_processed.get(
                        server_id, 
                        datetime.now() - timedelta(hours=24)
                    )
                    
                    # Discover files specifically for newest
                    csv_files, discovery_stats = await self.file_discovery.discover_csv_files(
                        sftp=sftp,
                        server_id=server_id,
                        start_date=start_date,
                        days_back=1,
                        historical_mode=False
                    )
                    
                    # Log discovery results
                    total_files = discovery_stats["total_files"]
                    filtered_files = discovery_stats["filtered_files"]
                    
                    logger.info(f"Killfeed discovery found {filtered_files} files since {start_date}")
                    
                    # Process each file incrementally from the last known position
                    for file_path in csv_files:
                        try:
                            # Skip files that have been fully processed
                            if self.parser.is_file_processed(file_path):
                                continue
                                
                            content = await sftp.read_file(file_path)
                            if not content:
                                logger.warning(f"Empty content for file {file_path}")
                                continue
                                
                            # Get the last processed line for this file
                            start_line = self.parser.get_last_processed_line(file_path)
                            
                            # Process from the last known position
                            events, total_lines = self.parser.parse_file_content(
                                content=content,
                                file_path=file_path,
                                server_id=server_id,
                                start_line=start_line
                            )
                            
                            # If we have events, process them
                            if events:
                                files_processed += 1
                                events_processed += len(events)
                                logger.info(f"Processed {len(events)} new events from {file_path}")
                                
                                # Process events (to be implemented by the cog)
                                await self._process_events(events, server_id)
                        except Exception as e:
                            logger.error(f"Error processing file {file_path}: {e}")
                            continue
                    
                    # Set the last processed time to now
                    self.last_processed[server_id] = datetime.now()
                    
                    # Update processing statistics
                    killfeed_stats = self.processing_stats.get(server_id, {})
                    killfeed_stats.update({
                        "killfeed_last_run": datetime.now(),
                        "killfeed_files_processed": files_processed,
                        "killfeed_events_processed": events_processed,
                        "killfeed_files_found": filtered_files
                    })
                    self.processing_stats[server_id] = killfeed_stats
                    
                    logger.info(f"Killfeed processing completed for server {server_id}. "
                              f"Processed {files_processed} files with {events_processed} events.")
                    
                    return files_processed, events_processed
                finally:
                    # Always release the lock
                    lock.release()
            else:
                logger.info(f"Skipping killfeed processing for server {server_id} - couldn't acquire lock")
                return 0, 0
        except asyncio.TimeoutError:
            logger.info(f"Skipping killfeed processing for server {server_id} - lock acquisition timed out")
            return 0, 0
    
    async def _process_events(self, events: List[Dict[str, Any]], server_id: str):
        """
        Process events extracted from CSV files.
        
        This is a placeholder method to be implemented by the cog.
        The coordinator itself doesn't process events, only extracts them.
        
        Args:
            events: List of events to process
            server_id: Server ID for these events
        """
        # This will be implemented by the cog, not here
        pass
    
    def get_processing_stats(self, server_id: str) -> Dict[str, Any]:
        """
        Get processing statistics for a server.
        
        Args:
            server_id: Server ID
            
        Returns:
            Dictionary of processing statistics
        """
        return self.processing_stats.get(server_id, {})
    
    def set_events_processor(self, processor_function):
        """
        Set the function to use for processing events.
        
        Args:
            processor_function: Async function that takes (events, server_id)
        """
        self._process_events = processor_function